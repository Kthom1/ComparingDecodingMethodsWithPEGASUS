{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ComparingDecodingMethodsWithPEGASUS.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNTFSJM1yabCfyra0apT/gh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kthom1/ComparingDecodingMethodsWithPEGASUS/blob/main/ComparingDecodingMethodsWithPEGASUS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09eIzhCSqNSI"
      },
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjK-1vZ5qhMk"
      },
      "source": [
        "\n",
        "# Always connect the drive first, so you can save relevant files for later\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zq312qRcqe1N"
      },
      "source": [
        "# Check your GPU\n",
        "!nvidia-smi -L"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ry3CmNpqkQT"
      },
      "source": [
        "# Import pytorch and print version, and check that you are in a Runtime with a GPU avaialable\n",
        "import torch\n",
        "import os\n",
        "# This helps with debugging cuda errors; offers stack trace\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "print(torch.__version__)\n",
        "\n",
        "# If this returns cpu, go to Runtime then Change Runtime type to GPU\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqNfXV23qoQR"
      },
      "source": [
        "# Huggingface\n",
        "# Transformers installation\n",
        "! pip install --upgrade transformers\n",
        "! pip install datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plPihLhVqtFS"
      },
      "source": [
        "import transformers\n",
        "print(transformers.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ar9iCa7_qw_J"
      },
      "source": [
        "from rouge import FilesRouge, Rouge\n",
        "\n",
        "rouge = Rouge()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MS6blkfhq3FC"
      },
      "source": [
        "import datasets\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgJBJ96YrNVu"
      },
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnH7x5P7we7_"
      },
      "source": [
        "# To measure lexical diversity\n",
        "!pip install lexical-diversity\n",
        "from lexical_diversity import lex_div as ld"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0J8CgE6qq8rQ"
      },
      "source": [
        "cnn_test_from_huggingface = datasets.load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"test[:1%]\")\n",
        "# {\n",
        "#   \"article\": \"string\",\n",
        "#   \"highlights\": \"string\"\n",
        "# }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75X5rqH0q-3N"
      },
      "source": [
        "cnn_df = pd.DataFrame(cnn_test_from_huggingface)\n",
        "cnn_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-h1HuO6drBPI"
      },
      "source": [
        "# Needed for pegasus\n",
        "!pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vh-usECkrDhV"
      },
      "source": [
        "# Pegasus Large\n",
        "pegasus_model_large = AutoModelForSeq2SeqLM.from_pretrained(\"google/pegasus-large\").to(device)\n",
        "pegasus_tokenizer_large = AutoTokenizer.from_pretrained(\"google/pegasus-large\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neCKfnY9rFxS"
      },
      "source": [
        "# Pegasus fine-tuned CNN\n",
        "pegasus_model_cnn_dm = AutoModelForSeq2SeqLM.from_pretrained(\"google/pegasus-cnn_dailymail\").to(device)\n",
        "pegasus_tokenizer_cnn_dm = AutoTokenizer.from_pretrained(\"google/pegasus-cnn_dailymail\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7WGzKIzrRmo"
      },
      "source": [
        "def pegasus_text_to_tensor_batch_with_tokenizer(text, relevant_tokenizer):\n",
        "  batch = relevant_tokenizer.prepare_seq2seq_batch(text, truncation=True, padding='longest', return_tensors='pt').to(device)\n",
        "  return batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rPTQ5E7rT7y"
      },
      "source": [
        "MAX_NUM_OUTPUT_TOKENS_SMALL = 128;\n",
        "MAX_NUM_OUTPUT_TOKENS_LARGE = 256"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTyMzdyqrWZS"
      },
      "source": [
        "def pegasus_text_to_text_with_model_tokenizer(text, relevant_model, relevant_tokenizer, max_num_output_tokens, num_beams=1, num_beam_groups=1, temperature=1.0, top_k=0, top_p=1.0, repetition_penalty=1.0, diversity_penalty=0.0, no_repeat_ngram_size=0, early_stopping=False, do_sample=False):\n",
        "  context = pegasus_text_to_tensor_batch_with_tokenizer(text, relevant_tokenizer)\n",
        "  input_ids = context['input_ids']\n",
        "  outputs = relevant_model.generate(input_ids, max_length=max_num_output_tokens, num_beams=num_beams, num_beam_groups=num_beam_groups, temperature=temperature, top_k=top_k, top_p=top_p, repetition_penalty=repetition_penalty, diversity_penalty=diversity_penalty, no_repeat_ngram_size=no_repeat_ngram_size, early_stopping=early_stopping, do_sample=do_sample)\n",
        "  text = relevant_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "  return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ietC4K6rZto"
      },
      "source": [
        "CNN_DM_DATASET_NAME = 'CNN-DM'\n",
        "CNN_DM_TEXT_IDENTIFIER = 'article'\n",
        "CNN_DM_SUMMARY_IDENTIFIER = 'highlights'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywgyInXdsRWq"
      },
      "source": [
        "PEGASUS_LARGE_MODEL_NAME = 'PEGASUS-LARGE'\n",
        "PEGASUS_CNN_DM_MODEL_NAME = 'PEGASUS-CNN-DM'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0qX64bMrfEi"
      },
      "source": [
        "def write_actual_summaries_to_file(dataframe, summaryIdentifier, dataset_name, num_articles=1):\n",
        "  actual_summaries = open('{}_dataset-name_{}_num-articles_actual_summaries.txt'.format(dataset_name, num_articles), 'w')\n",
        "\n",
        "  for row in dataframe.head(num_articles).itertuples():\n",
        "    actual_summary = getattr(row, summaryIdentifier)\n",
        "    actual_summaries.write(actual_summary + '\\n\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78pP87YosL8h"
      },
      "source": [
        "# WRITE ACTUAL SUMMARIES\n",
        "write_actual_summaries_to_file(dataframe=cnn_df, summaryIdentifier=CNN_DM_SUMMARY_IDENTIFIER, dataset_name=CNN_DM_DATASET_NAME, num_articles=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifNh68-UsinP"
      },
      "source": [
        "NUM_BEAMS = 8\n",
        "MORE_BEAMS = 16"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDhP-vhQsmKw"
      },
      "source": [
        "TOP_K = 40\n",
        "MORE_K = 640\n",
        "TEMPERATURE = 0.7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdJhypgzsp9W"
      },
      "source": [
        "# NUCLEUS SAMPLE\n",
        "NUCLEUS_SAMPLE_VALUE = 0.95"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sMZJiBLswyK"
      },
      "source": [
        "def generate_all_summaries(dataframe, model, model_name, tokenizer, textIdentifier, dataset_name, num_articles=100):\n",
        "  approach_name = 'abstractive'\n",
        "\n",
        "  beam_search_summaries_small = open('{}_dataset-name_BEAM-SEARCH_{}_model-name_{}_approach-name_{}_num-articles_{}_num-beams_generated_summaries.txt'.format(dataset_name, model_name, approach_name, num_articles, NUM_BEAMS), 'w')\n",
        "  beam_search_summaries_large = open('{}_dataset-name_BEAM-SEARCH_{}_model-name_{}_approach-name_{}_num-articles_{}_num-beams_generated_summaries.txt'.format(dataset_name, model_name, approach_name, num_articles, MORE_BEAMS), 'w')\n",
        "\n",
        "  top_k_summaries_small = open('{}_dataset-name_TOP-K_{}_model-name_{}_approach-name_{}_num-articles_{}_top-k_{}_do-sample_generated_summaries.txt'.format(dataset_name, model_name, approach_name, num_articles, TOP_K, True), 'w')\n",
        "  top_k_summaries_large = open('{}_dataset-name_TOP-K_{}_model-name_{}_approach-name_{}_num-articles_{}_top-k_{}_do-sample_generated_summaries.txt'.format(dataset_name, model_name, approach_name, num_articles, MORE_K, True), 'w')\n",
        "\n",
        "  top_k_temperature_summaries_small = open('{}_dataset-name_TOP-K-TEMPERATURE_{}_model-name_{}_approach-name_{}_top-k_{}_temperature_{}_do-sample_generated_summaries.txt'.format(dataset_name, model_name, approach_name, num_articles, TOP_K, TEMPERATURE, True), 'w')\n",
        "  top_k_temperature_summaries_large = open('{}_dataset-name_TOP-K-TEMPERATURE_{}_model-name_{}_approach-name_{}_top-k_{}_temperature_{}_do-sample_generated_summaries.txt'.format(dataset_name, model_name, approach_name, num_articles, MORE_K, TEMPERATURE, True), 'w')\n",
        "\n",
        "  top_p_summaries = open('{}_dataset-name_TOP-P_{}_model-name_{}_approach-name_{}_num-articles_{}_top-p_{}_do-sample_generated_summaries.txt'.format(dataset_name, model_name, approach_name, num_articles, NUCLEUS_SAMPLE_VALUE, True), 'w')\n",
        "\n",
        "  for row in dataframe.head(num_articles).itertuples():\n",
        "    input_text = getattr(row, textIdentifier)\n",
        "    final_input_text = input_text\n",
        "\n",
        "    # BEAM SMALL \n",
        "    beam_search_summary_small = pegasus_text_to_text_with_model_tokenizer(text=final_input_text, relevant_model=model, relevant_tokenizer=tokenizer, max_num_output_tokens=MAX_NUM_OUTPUT_TOKENS_SMALL, num_beams=NUM_BEAMS)\n",
        "    beam_search_summaries_small.write(beam_search_summary_small + '\\n\\n')\n",
        "    # BEAM LARGE \n",
        "    beam_search_summary_large = pegasus_text_to_text_with_model_tokenizer(text=final_input_text, relevant_model=model, relevant_tokenizer=tokenizer, max_num_output_tokens=MAX_NUM_OUTPUT_TOKENS_SMALL, num_beams=MORE_BEAMS)\n",
        "    beam_search_summaries_large.write(beam_search_summary_large + '\\n\\n')\n",
        "    # TOP K SMALL\n",
        "    top_k_summary_small = pegasus_text_to_text_with_model_tokenizer(text=final_input_text, relevant_model=model, relevant_tokenizer=tokenizer, max_num_output_tokens=MAX_NUM_OUTPUT_TOKENS_SMALL, top_k=TOP_K, do_sample=True)\n",
        "    top_k_summaries_small.write(top_k_summary_small + '\\n\\n')\n",
        "    # TOP K LARGE \n",
        "    top_k_summary_large = pegasus_text_to_text_with_model_tokenizer(text=final_input_text, relevant_model=model, relevant_tokenizer=tokenizer, max_num_output_tokens=MAX_NUM_OUTPUT_TOKENS_SMALL, top_k=MORE_K, do_sample=True)\n",
        "    top_k_summaries_large.write(top_k_summary_large + '\\n\\n')\n",
        "    # TOP K TEMPERATURE SMALL\n",
        "    top_k_temperature_summary_small = pegasus_text_to_text_with_model_tokenizer(text=final_input_text, relevant_model=model, relevant_tokenizer=tokenizer, max_num_output_tokens=MAX_NUM_OUTPUT_TOKENS_SMALL, top_k=TOP_K, temperature=TEMPERATURE, do_sample=True)\n",
        "    top_k_temperature_summaries_small.write(top_k_temperature_summary_small + '\\n\\n')\n",
        "    # TOP K TEMPERATURE LARGE \n",
        "    top_k_temperature_summary_large = pegasus_text_to_text_with_model_tokenizer(text=final_input_text, relevant_model=model, relevant_tokenizer=tokenizer, max_num_output_tokens=MAX_NUM_OUTPUT_TOKENS_SMALL, top_k=MORE_K, temperature=TEMPERATURE, do_sample=True)\n",
        "    top_k_temperature_summaries_large.write(top_k_temperature_summary_large + '\\n\\n')\n",
        "    # TOP P\n",
        "    top_p_summary = pegasus_text_to_text_with_model_tokenizer(text=final_input_text, relevant_model=model, relevant_tokenizer=tokenizer, max_num_output_tokens=MAX_NUM_OUTPUT_TOKENS_SMALL, top_p=NUCLEUS_SAMPLE_VALUE, do_sample=True)\n",
        "    top_p_summaries.write(top_p_summary + '\\n\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsJ8SEJMtLmR"
      },
      "source": [
        "# CNN ALL  LARGE\n",
        "generate_all_summaries(cnn_df, model=pegasus_model_large, model_name=PEGASUS_LARGE_MODEL_NAME, tokenizer=pegasus_tokenizer_large, textIdentifier=CNN_DM_TEXT_IDENTIFIER, dataset_name=CNN_DM_DATASET_NAME)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG51BpYetOmS"
      },
      "source": [
        "# CNN ALL ABSTRACTIVE CNN FINE-TUNED\n",
        "generate_all_summaries(cnn_df, model=pegasus_model_cnn_dm, model_name=PEGASUS_CNN_DM_MODEL_NAME, tokenizer=pegasus_tokenizer_cnn_dm, textIdentifier=CNN_DM_TEXT_IDENTIFIER, dataset_name=CNN_DM_DATASET_NAME)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eb-Z_hETtXH2"
      },
      "source": [
        "import os\n",
        "\n",
        "with open(cnn_actual_summaries_file_path, 'r') as cnn_actual_summaries_file:\n",
        "  cnn_actual_summaries_data = cnn_actual_summaries_file.read()\n",
        "\n",
        "  flt = ld.flemmatize(cnn_actual_summaries_data)\n",
        "\n",
        "  mtld = ld.mtld(flt)\n",
        "\n",
        "  print(\"LD - MTLD\")\n",
        "\n",
        "  print(mtld)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJyg_CJPtw8Y"
      },
      "source": [
        "import os\n",
        "\n",
        "def loop_directory_and_write_scores(directory, actual_summaries_file_path, dataset_name):\n",
        "  print(dataset_name)\n",
        "\n",
        "  with open(actual_summaries_file_path, 'r') as actual_summaries_file:\n",
        "    actual_summaries_data = actual_summaries_file.read()\n",
        "\n",
        "  for entry in os.scandir(directory):\n",
        "    if (entry.path.endswith('.txt')):\n",
        "      generated_summaries_file_path = entry.path\n",
        "\n",
        "      with open(generated_summaries_file_path, 'r') as generated_summaries_file:\n",
        "        generated_summaries_data = generated_summaries_file.read()\n",
        "\n",
        "      \n",
        "      rouge_scores_avg = rouge.get_scores(generated_summaries_data, actual_summaries_data, avg=True)\n",
        "\n",
        "      flt = ld.flemmatize(generated_summaries_data)\n",
        "\n",
        "      mtld = ld.mtld(flt)\n",
        "\n",
        "      hdd = ld.hdd(flt)\n",
        "\n",
        "      print(\"PATH\")\n",
        "\n",
        "      print(entry.path)\n",
        "\n",
        "\n",
        "      print(\"ROUGE SCORE AVG\")\n",
        "\n",
        "      print(rouge_scores_avg)\n",
        "\n",
        "      print(\"LD - MTLD\")\n",
        "\n",
        "      print(mtld)\n",
        "\n",
        "      print(\"LD - HDD\")\n",
        "\n",
        "      print(hdd, end='\\n\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HHzuRm-uK4t"
      },
      "source": [
        "# Get scores\n",
        "loop_directory_and_write_scores(directory='./DIRECTORY_WITH_ALL_YOUR_GENERATED SUMMARIES', actual_summaries_file_path='./CNN-DM_dataset-name_100_num-articles_actual_summaries.txt', dataset_name=CNN_DM_DATASET_NAME)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}